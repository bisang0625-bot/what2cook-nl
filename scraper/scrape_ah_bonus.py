"""
Albert Heijn ê³µì‹ Bonus í˜ì´ì§€ í¬ë¡¤ë§
ì‹¤ì œ ì„¸ì¼ ìƒí’ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
"""
import os
import json
from pathlib import Path
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import re
import time
from datetime import datetime, timedelta

# ë¸Œë¼ìš°ì € ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent
LOCAL_BROWSERS_PATH = PROJECT_ROOT / "pw-browsers"
if LOCAL_BROWSERS_PATH.exists():
    os.environ['PLAYWRIGHT_BROWSERS_PATH'] = str(LOCAL_BROWSERS_PATH)

def scrape_ah_bonus():
    """Albert Heijn Bonus í˜ì´ì§€ì—ì„œ ì„¸ì¼ ìƒí’ˆ í¬ë¡¤ë§"""
    url = "https://www.ah.nl/bonus"
    products = []
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080}
        )
        page = context.new_page()
        
        print(f"ğŸ” Albert Heijn Bonus í˜ì´ì§€ ì ‘ì†: {url}")
        page.goto(url, timeout=60000)
        page.wait_for_load_state("networkidle")
        
        # ì¿ í‚¤ ë™ì˜ ì²˜ë¦¬
        try:
            # AH ì‚¬ì´íŠ¸ì˜ ì¿ í‚¤ ë™ì˜ ë²„íŠ¼
            cookie_selectors = [
                "button:has-text('Accepteren')",
                "button:has-text('Akkoord')",
                "button:has-text('Accept')",
                "[data-testid='accept-cookies']",
                "#accept-cookies"
            ]
            for selector in cookie_selectors:
                try:
                    btn = page.locator(selector).first
                    if btn.is_visible(timeout=3000):
                        print("ğŸª ì¿ í‚¤ ë™ì˜ ë²„íŠ¼ í´ë¦­")
                        btn.click()
                        time.sleep(2)
                        break
                except:
                    continue
        except:
            pass
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        time.sleep(5)
        
        # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ìƒí’ˆ ë¡œë”©
        print("ğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì¤‘...")
        for _ in range(3):
            page.evaluate("window.scrollBy(0, window.innerHeight)")
            time.sleep(1)
        
        # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
        screenshot_path = PROJECT_ROOT / "data" / "ah_bonus_screenshot.png"
        page.screenshot(path=str(screenshot_path), full_page=True)
        print(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
        
        # HTML ê°€ì ¸ì˜¤ê¸°
        content = page.content()
        
        # HTML ì €ì¥
        html_path = PROJECT_ROOT / "data" / "ah_bonus_page.html"
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"ğŸ“„ HTML ì €ì¥: {html_path}")
        
        soup = BeautifulSoup(content, 'html.parser')
        
        # í˜ì´ì§€ ì œëª© í™•ì¸
        title = page.title()
        print(f"ğŸ“° í˜ì´ì§€ ì œëª©: {title}")
        
        # ìƒí’ˆ ì¹´ë“œ ì°¾ê¸° - AH ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ
        # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        product_selectors = [
            '[data-testhook="product-card"]',
            '[data-testhook="bonus-card"]',
            'article[class*="product"]',
            'div[class*="product-card"]',
            'div[class*="ProductCard"]',
            'a[href*="/producten/"]',
        ]
        
        found_products = []
        for selector in product_selectors:
            elements = soup.select(selector)
            if elements:
                print(f"âœ… '{selector}' ì„ íƒìë¡œ {len(elements)}ê°œ ìš”ì†Œ ë°œê²¬")
                found_products.extend(elements)
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_products = []
        for elem in found_products:
            elem_str = str(elem)[:200]
            if elem_str not in seen:
                seen.add(elem_str)
                unique_products.append(elem)
        
        print(f"ğŸ“¦ ì´ {len(unique_products)}ê°œì˜ ê³ ìœ  ìƒí’ˆ ìš”ì†Œ ë°œê²¬")
        
        # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
        for elem in unique_products[:50]:  # ìµœëŒ€ 50ê°œ
            try:
                product = extract_product_info(elem)
                if product:
                    products.append(product)
            except Exception as e:
                continue
        
        # í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ (ë°±ì—…)
        if len(products) < 5:
            print("âš ï¸ ìƒí’ˆì´ ì ê²Œ ë°œê²¬ë¨, í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹œë„...")
            all_text = soup.get_text()
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰
            keywords = ['speklappen', 'kip', 'varken', 'gehakt', 'kaas', 'melk', 'brood', 
                       'pasta', 'druiven', 'vis', 'vlees', 'groente', 'aardappel', 'ui', 'tomaat']
            for kw in keywords:
                if kw.lower() in all_text.lower():
                    print(f"  âœ… '{kw}' í…ìŠ¤íŠ¸ ë°œê²¬!")
        
        browser.close()
    
    # ê²°ê³¼ ì €ì¥
    result = {
        'scraped_at': datetime.now().isoformat(),
        'source': 'ah.nl/bonus',
        'supermarket': 'Albert Heijn',
        'total_products': len(products),
        'products': products
    }
    
    output_path = PROJECT_ROOT / "data" / "ah_bonus_products.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… {len(products)}ê°œ ìƒí’ˆ ì €ì¥: {output_path}")
    
    return products

def extract_product_info(elem):
    """ìƒí’ˆ ìš”ì†Œì—ì„œ ì •ë³´ ì¶”ì¶œ"""
    try:
        # ìƒí’ˆëª… ì¶”ì¶œ
        name = None
        name_selectors = ['h3', 'h4', 'h2', '[class*="title"]', '[class*="name"]', 'strong']
        for sel in name_selectors:
            name_elem = elem.select_one(sel)
            if name_elem:
                name = name_elem.get_text(strip=True)
                if name and len(name) > 2:
                    break
        
        if not name:
            # ë§í¬ í…ìŠ¤íŠ¸ ì‚¬ìš©
            link = elem.select_one('a')
            if link:
                name = link.get_text(strip=True)
        
        if not name or len(name) < 3:
            return None
        
        # ê°€ê²© ì¶”ì¶œ
        price = None
        price_selectors = ['[class*="price"]', '[class*="Price"]', 'span[class*="euro"]']
        for sel in price_selectors:
            price_elem = elem.select_one(sel)
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                price_match = re.search(r'[\d,.]+', price_text)
                if price_match:
                    price = price_match.group()
                    break
        
        # í• ì¸ ì •ë³´ ì¶”ì¶œ
        discount = None
        discount_selectors = ['[class*="discount"]', '[class*="bonus"]', '[class*="action"]']
        for sel in discount_selectors:
            discount_elem = elem.select_one(sel)
            if discount_elem:
                discount = discount_elem.get_text(strip=True)
                break
        
        return {
            'name': name,
            'price': price,
            'discount_info': discount,
            'supermarket': 'Albert Heijn'
        }
    except:
        return None

if __name__ == "__main__":
    products = scrape_ah_bonus()
    print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! {len(products)}ê°œ ìƒí’ˆ ìˆ˜ì§‘")
    
    # ìƒìœ„ 10ê°œ ìƒí’ˆ ì¶œë ¥
    print("\nğŸ“‹ ìƒìœ„ 10ê°œ ìƒí’ˆ:")
    for i, p in enumerate(products[:10], 1):
        print(f"  {i}. {p['name']} - {p.get('price', 'N/A')} - {p.get('discount_info', '')}")
